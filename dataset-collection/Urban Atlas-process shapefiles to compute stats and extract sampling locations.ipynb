{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Urban Atlas - process shapefiles to compute stats and extract sampling locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook processess the collection of GIS vector data (shapefiles) on land use classses from the Urban Atlas dataset. The tasks we are interested in are:\n",
    "\n",
    "* compute basic statistics on each shapefile, such as the spatial extent of the city and the amount of land classified within each city\n",
    "* construct ground truth test grids (defined as a $25km \\times 25km$ square window around the city center)\n",
    "* define locations to sample images, both for testing (each cell of the ground truth grid) and for training (sampled appropriately from all available data)\n",
    "\n",
    "In this analysis we work directly with vector data (polygons) for all the above tasks. This is a faster way to process the data for our specific use case of whole-image classification, as opposed to the more accurate approach of rasterizing the data first, then computing classification masks (which is more suited for image segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: packages, paths etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel\n",
    "\n",
    "rc = ipyparallel.Client()\n",
    "all_engines = rc[:]\n",
    "lbv = rc.load_balanced_view()\n",
    "\n",
    "print len(all_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "# numeric packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# filesystem and OS\n",
    "import sys, os, time\n",
    "import glob\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "# compression\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "import copy\n",
    "\n",
    "# widgets and interaction\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# these magics ensure that external modules that are modified are also automatically reloaded\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "# path to shapefiles\n",
    "\n",
    "shapefiles_path = \"/home/data/urban-atlas/shapefiles/\"\n",
    "\n",
    "shapefiles = glob.glob(\"%s/*/*/*.shp\"%shapefiles_path)\n",
    "shapefiles = {\" \".join(f.split(\"/\")[-1].split(\"_\")[1:]).replace(\".shp\",\"\"):f for f in shapefiles}\n",
    "\n",
    "# path to save data\n",
    "\n",
    "outPath = \"/home/data/urban-atlas/extracted-data\"\n",
    "\n",
    "if not os.path.exists(outPath):\n",
    "    os.makedirs(outPath)\n",
    "    \n",
    "# classess used in the Urban Atlas dataset\n",
    "\n",
    "classes = '''Agricultural + Semi-natural areas + Wetlands\n",
    "Airports\n",
    "Construction sites\n",
    "Continuous Urban Fabric (S.L. > 80%)\n",
    "Discontinuous Dense Urban Fabric (S.L. : 50% -  80%)\n",
    "Discontinuous Low Density Urban Fabric (S.L. : 10% - 30%)\n",
    "Discontinuous Medium Density Urban Fabric (S.L. : 30% - 50%)\n",
    "Discontinuous Very Low Density Urban Fabric (S.L. < 10%)\n",
    "Fast transit roads and associated land\n",
    "Forests\n",
    "Green urban areas\n",
    "Industrial, commercial, public, military and private units\n",
    "Isolated Structures\n",
    "Land without current use\n",
    "Mineral extraction and dump sites\n",
    "Other roads and associated land\n",
    "Port areas\n",
    "Railways and associated land\n",
    "Sports and leisure facilities\n",
    "Water bodies'''.split(\"\\n\")\n",
    "\n",
    "class2label = {c:i for i,c in enumerate(classes)}\n",
    "label2class = {i:c for i,c in enumerate(classes)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "# satellite imagery modules\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/nbserver/urban-environments/urbanatlas/\")\n",
    "import urbanatlas as ua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct ground truth rasters for validation\n",
    "\n",
    "Also compute useful stats within windows of L=25,30,50km around the city center:\n",
    "* percentage of polygons per class \n",
    "* percentage of classified area per class\n",
    "* percentage of classified area vs total area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12292 polygons | 18 land use classes\n"
     ]
    }
   ],
   "source": [
    "myname = \"bucuresti\"\n",
    "\n",
    "# read in shapefile\n",
    "shapefile = shapefiles[city]\n",
    "\n",
    "mycity = ua.UAShapeFile(shapefile, name=myname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print \"Spatial extent: %2.2f km.\" % L\n",
    "    print \"Land use classified area: %2.3f km^2 (%2.2f of total area covered within bounds %2.3f km^2)\"%(classified_area, frac_classified, box_area)\n",
    "    \n",
    "    return L, frac_classified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "grid_cell = 100\n",
    "grid_size = (grid_cell, grid_cell)\n",
    "window_km_vec = [25, 30, 50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fn_generate_stats(shapefile):\n",
    "    city = \" \".join(shapefile.split(\"/\")[-1].split(\"_\")[1:]).replace(\".shp\",\"\")\n",
    "    \n",
    "    # weird issues with several cities, skip\n",
    "    if city in [\"limoges\", \"linz\"]:\n",
    "        return \"Error for city %s\"%city\n",
    "    \n",
    "    print \"Processing %s\"%city\n",
    "    \n",
    "    savedir = \"%s/%s/\"%(outPath, city)\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "\n",
    "    if len([x for x in os.listdir(savedir) if 'raster' in x])==3:\n",
    "        return \"Already processed!\"\n",
    "   \n",
    "    gdf, prj = load_shapefile(shapefile)\n",
    "    if gdf is None:\n",
    "        return \"Error reading shapefile %s\"%shapefile\n",
    "        \n",
    "    city_center, country_code = get_city_center(shapefile)\n",
    "    lonmin, latmin, lonmax, latmax = get_bounds(gdf)\n",
    "    bounds_gdf = Polygon([(lonmin,latmin), (lonmax,latmin), (lonmax,latmax), (lonmin,latmax)])\n",
    "\n",
    "    if city_center is None:\n",
    "        city_center = ((latmin+latmax)/2.0, (lonmin+lonmax)/2.0)\n",
    "\n",
    "    # there's some weird issue with the shapefile for Graz\n",
    "    # lat and lon are inverted?\n",
    "    if city in [\"graz\"]: #not bounds_gdf.contains(Point(city_center[::-1])):\n",
    "        city_center = ((latmin+latmax)/2.0, (lonmin+lonmax)/2.0)\n",
    "        gdf['geometry'] = gdf['geometry'].apply(\\\n",
    "                lambda p: Polygon((lon,lat) \\\n",
    "                    for (lon,lat) in zip(p.exterior.coords.xy[1], p.exterior.coords.xy[0])))\n",
    "    \n",
    "    # compute spatial extent of city and fraction of land classified\n",
    "    L, frac_classified = compute_stats(gdf, prj=prj)\n",
    "    df = pd.DataFrame([L, frac_classified], \\\n",
    "                      index=[\"spatial extent\", \"pct land classified\"]).T\n",
    "    df.to_csv(\"%s/basic_stats.csv\"%savedir)\n",
    "        \n",
    "    for window_km in window_km_vec:\n",
    "        window = (window_km, window_km)\n",
    "        gdf_window = filter_gdf_by_centered_window(gdf, center=city_center, window=window)\n",
    "        \n",
    "        # compute stats\n",
    "        class_coverage_by_area = gdf_window.groupby(\"ITEM\").apply(\\\n",
    "                                lambda x: x[\"SHAPE_AREA\"].sum())/float(window[0]*window[1])\n",
    "        class_coverage_by_poly= gdf_window.groupby(\"ITEM\").apply(len)/ gdf.groupby(\"ITEM\").apply(len)\n",
    "        class_coverage_by_area_classified = gdf_window.groupby(\"ITEM\").apply(\\\n",
    "                                                lambda x: x['SHAPE_AREA'].sum()) / gdf_window['SHAPE_AREA'].sum()\n",
    "    \n",
    "        # format and save stats\n",
    "        stats_df = pd.concat([class_coverage_by_area, class_coverage_by_poly, class_coverage_by_area_classified], axis=1)\n",
    "        stats_df.columns = [\"pct area\", \"pct polygons\", \"pct classified area\"]\n",
    "        stats_df['window km'] = window_km\n",
    "        stats_df = stats_df.ix[classes]\n",
    "        stats_df.to_csv(\"%s/stats_class_window_%d.csv\"%(savedir,window_km))\n",
    "        \n",
    "        # compute raster for given window size\n",
    "        bbox = satimg.bounding_box_at_location(city_center, window)\n",
    "        raster, locations_df = construct_class_raster(gdf_window, bbox, grid_size=grid_size)\n",
    "        np.savez_compressed(\"%s/ground_truth_class_raster_%d.npz\"%(savedir,window_km), raster)\n",
    "        locations_df.to_csv(\"%s/sample_locations_raster_%d.csv\"%(savedir,window_km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_center, country_code = get_city_center(shapefile)\n",
    "# lonmin, latmin, lonmax, latmax = get_bounds(gdf)\n",
    "# bounds_gdf = Polygon([(lonmin,latmin), (lonmax,latmin), (lonmax,latmax), (lonmin,latmax)])\n",
    "# window = (window_km_vec[0], window_km_vec[0])\n",
    "# gdf_window = filter_gdf_by_centered_window(gdf, center=city_center, window=window)\n",
    "# bbox = satimg.bounding_box_at_location(city_center, window)\n",
    "# raster, locations_df = construct_class_raster(gdf_window, bbox, grid_size=grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = lbv.map_async(fn_generate_stats, shapefiles.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics on all ~300 cities in Urban Atlas\n",
    "\n",
    "Use computation results from the separate notebook \"Urban Atlas - generate sampling locations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on classified area and spatial extent of the city\n",
    "\n",
    "* What is the total area covered by the land use classification? \n",
    "* How does it break down into the different classes?\n",
    "* What is the spatial extent scale of the city?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapath = \"/home/data/urban-atlas/extracted-data/\"\n",
    "\n",
    "stats_files = glob.glob(\"%s/*/basic_stats.csv\"%datapath)\n",
    "\n",
    "basic_stats_df = pd.concat([pd.read_csv(f) for f in stats_files]).drop(\"Unnamed: 0\", 1)\n",
    "basic_stats_df.index = [f.split(\"/\")[-2] for f in stats_files]\n",
    "basic_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 1})\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "g = sns.jointplot(x=\"spatial extent\", y=\"pct land classified\", data=basic_stats_df, \\\n",
    "                  kind=\"kde\", color=\"k\", ylim=(0.2,0.9), xlim=(1,120), stat_func=None)\n",
    "g.plot_joMt.collections[0].set_alpha(0)\n",
    "g.set_axis_labels(\"city spatial extent $L$ [km]\", \"land classified $\\%$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate locations to extract imagery at\n",
    "\n",
    "Our sampling strategy has the following goals:\n",
    "* ensure that a uniform $100 \\times 100 ~ (25km \\times 25km)$ \"main grid\" is completely sampled (except for where there are no ground truth polygons). We generate samples in this grid first, and assign the ground truth label of the image sampled in each grid cell to the class of the polygon that has the maximum intersection area with that cell; \n",
    "* ensure that the resulting dataset is balanced with respect to the land use classes. The trouble is that the classes are highly imbalanced among the polygons in the dataset (e.g., many more polygons are agricultural land and isolated structures than airports).\n",
    "* sample additional polygons apart from the ones in the initial grid, such that only polygons above a certain threshold size are considered (so that we can ensure that the sampled images contain a large enough area of the class they represent). \n",
    "* to ensure higher match between labels and sampled images, sample more images from polygons of larger areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
