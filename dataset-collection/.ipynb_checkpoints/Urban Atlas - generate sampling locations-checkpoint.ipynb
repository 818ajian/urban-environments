{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for connection file: ~/.ipython/profile_default/security/ipcontroller-client.json\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "Connection file '~/.ipython/profile_default/security/ipcontroller-client.json' not found.\nYou have attempted to connect to an IPython Cluster but no Controller could be found.\nPlease double-check your configuration and ensure that a cluster is running.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-00b559978617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mipyparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mipyparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mall_engines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlbv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_balanced_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/ipyparallel/client/client.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url_file, profile, profile_dir, ipython_dir, context, debug, sshserver, sshkey, password, paramiko, timeout, cluster_id, **extra_args)\u001b[0m\n\u001b[1;32m    395\u001b[0m                         \u001b[0mno_file_msg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                     ])\n\u001b[0;32m--> 397\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0murl_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_file_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Connection file '~/.ipython/profile_default/security/ipcontroller-client.json' not found.\nYou have attempted to connect to an IPython Cluster but no Controller could be found.\nPlease double-check your configuration and ensure that a cluster is running."
     ]
    }
   ],
   "source": [
    "import ipyparallel\n",
    "\n",
    "rc = ipyparallel.Client()\n",
    "all_engines = rc[:]\n",
    "lbv = rc.load_balanced_view()\n",
    "\n",
    "print len(all_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "# numeric packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# filesystem and OS\n",
    "import sys, os, time\n",
    "import glob\n",
    "\n",
    "# plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "# compression\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "import copy\n",
    "\n",
    "# geo stuff\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# widgets and interaction\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# these magics ensure that external modules that are modified are also automatically reloaded\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "# path to shapefiles\n",
    "\n",
    "shapefiles_path = \"/home/adalbert/data/urban-atlas/shapefiles/\"\n",
    "\n",
    "shapefiles = glob.glob(\"%s/*/*/*.shp\"%shapefiles_path)\n",
    "shapefiles = {\" \".join(f.split(\"/\")[-1].split(\"_\")[1:]).replace(\".shp\",\"\"):f for f in shapefiles}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "# path to save data\n",
    "\n",
    "outPath = \"/home/adalbert/data/urban-atlas/extracted-data\"\n",
    "\n",
    "if not os.path.exists(outPath):\n",
    "    os.makedirs(outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "classes = '''Agricultural + Semi-natural areas + Wetlands\n",
    "Airports\n",
    "Construction sites\n",
    "Continuous Urban Fabric (S.L. > 80%)\n",
    "Discontinuous Dense Urban Fabric (S.L. : 50% -  80%)\n",
    "Discontinuous Low Density Urban Fabric (S.L. : 10% - 30%)\n",
    "Discontinuous Medium Density Urban Fabric (S.L. : 30% - 50%)\n",
    "Discontinuous Very Low Density Urban Fabric (S.L. < 10%)\n",
    "Fast transit roads and associated land\n",
    "Forests\n",
    "Green urban areas\n",
    "Industrial, commercial, public, military and private units\n",
    "Isolated Structures\n",
    "Land without current use\n",
    "Mineral extraction and dump sites\n",
    "Other roads and associated land\n",
    "Port areas\n",
    "Railways and associated land\n",
    "Sports and leisure facilities\n",
    "Water bodies'''.split(\"\\n\")\n",
    "\n",
    "class2label = {c:i for i,c in enumerate(classes)}\n",
    "label2class = {i:c for i,c in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct ground truth rasters for validation\n",
    "\n",
    "Also compute useful stats within windows of L=25,30,50km around the city center:\n",
    "* percentage of polygons per class \n",
    "* percentage of classified area per class\n",
    "* percentage of classified area vs total area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "# satellite imagery modules\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/adalbert/nbserver/satellite-image-tools/satimage-processing/\")\n",
    "import satimg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "def load_shapefile(shapefile):\n",
    "    # read in shapefile\n",
    "    try:\n",
    "        gdf = gpd.GeoDataFrame.from_file(shapefile)\n",
    "    except:\n",
    "        print \"--> %s: error reading file!\"%shapefile\n",
    "        return None, None\n",
    "\n",
    "    city = shapefile.split(\"/\")[-1].split(\"_\")[1]\n",
    "    gdf.columns = [c.upper() if c != \"geometry\" else c for c in gdf.columns ]\n",
    "    if 'SHAPE_AREA' not in gdf.columns:\n",
    "        gdf['SHAPE_AREA'] = gdf['geometry'].apply(lambda p: p.area)\n",
    "    if 'SHAPE_LEN' not in gdf.columns:\n",
    "        gdf['SHAPE_LEN'] = gdf['geometry'].apply(lambda p: p.length)\n",
    "        \n",
    "    # convert area & length to km\n",
    "    gdf['SHAPE_AREA'] = gdf['SHAPE_AREA'] / 1.0e6 # convert to km^2\n",
    "    gdf['SHAPE_LEN']  = gdf['SHAPE_LEN'] / 1.0e3 # convert to km\n",
    "\n",
    "    classes = gdf['ITEM'].unique()\n",
    "    print \"%s: %d polygons | %d land use classes\" % (city, len(gdf), len(classes))\n",
    "\n",
    "    # read in projection file associated with shapefile\n",
    "    prjfile = shapefile.replace(\".shp\", \".prj\")\n",
    "    prj = satimg.read_prj(prjfile)   \n",
    "    \n",
    "    # change coordinate system from northing/easting to lonlat\n",
    "    targetcrs = {u'ellps': u'WGS84', u'datum': u'WGS84', u'proj': u'longlat'}\n",
    "    gdf.to_crs(crs=targetcrs, inplace=True)\n",
    "\n",
    "    return gdf, prj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucuresti.shp: 12292 polygons | 18 land use classes\n"
     ]
    }
   ],
   "source": [
    "city = \"bucuresti\"\n",
    "\n",
    "# read in shapefile\n",
    "shapefile = shapefiles[city]\n",
    "\n",
    "gdf, prj = load_shapefile(shapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Continuous Urban Fabric (S.L. > 80%)                            7126\n",
       "Industrial, commercial, public, military and private units      1978\n",
       "Agricultural + Semi-natural areas + Wetlands                     797\n",
       "Discontinuous Dense Urban Fabric (S.L. : 50% -  80%)             592\n",
       "Land without current use                                         359\n",
       "Green urban areas                                                329\n",
       "Isolated Structures                                              310\n",
       "Construction sites                                               181\n",
       "Water bodies                                                     173\n",
       "Sports and leisure facilities                                    140\n",
       "Forests                                                          128\n",
       "Mineral extraction and dump sites                                 59\n",
       "Other roads and associated land                                   50\n",
       "Railways and associated land                                      34\n",
       "Discontinuous Medium Density Urban Fabric (S.L. : 30% - 50%)      25\n",
       "Discontinuous Low Density Urban Fabric (S.L. : 10% - 30%)          4\n",
       "Airports                                                           4\n",
       "Fast transit roads and associated land                             3\n",
       "Name: ITEM, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf['ITEM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "def get_bounds(gdf):\n",
    "    bounds = np.array(gdf['geometry'].apply(lambda p: list(p.bounds)).values.tolist())\n",
    "    xmin = bounds[:,[0,2]].min()\n",
    "    xmax = bounds[:,[0,2]].max()\n",
    "    ymin = bounds[:,[1,3]].min()\n",
    "    ymax = bounds[:,[1,3]].max()\n",
    "    return xmin, ymin, xmax, ymax\n",
    "\n",
    "\n",
    "def compute_stats(gdf, prj=\"\"):\n",
    "    ''' \n",
    "    Statistics about the polygons in the geo data frame.\n",
    "    '''\n",
    "    lonmin, latmin, lonmax, latmax = get_bounds(gdf)\n",
    "    xmin, ymin = satimg.lonlat2xy((lonmin, latmin), prj=prj)\n",
    "    xmax, ymax = satimg.lonlat2xy((lonmax, latmax), prj=prj)\n",
    "\n",
    "    box_area =  (xmax-xmin) / 1.0e3 * (ymax-ymin) / 1.0e3\n",
    "    L = np.sqrt((xmax-xmin)**2 + (ymax-ymin)**2) / 1.0e3 / np.sqrt(2)\n",
    "    classified_area = gdf['SHAPE_AREA'].sum()\n",
    "    frac_classified = classified_area/box_area\n",
    "\n",
    "    print \"Spatial extent: %2.2f km.\" % L\n",
    "    print \"Land use classified area: %2.3f km^2 (%2.2f of total area covered within bounds %2.3f km^2)\"%(classified_area, frac_classified, box_area)\n",
    "    \n",
    "    return L, frac_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial extent: 79.73 km.\n",
      "Land use classified area: 1799.517 km^2 (0.35 of total area covered within bounds 5116.257 km^2)\n"
     ]
    }
   ],
   "source": [
    "L, frac_classified = compute_stats(gdf, prj=prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Agricultural + Semi-natural areas + Wetlands',\n",
       " 1: 'Airports',\n",
       " 2: 'Construction sites',\n",
       " 3: 'Continuous Urban Fabric (S.L. > 80%)',\n",
       " 4: 'Discontinuous Dense Urban Fabric (S.L. : 50% -  80%)',\n",
       " 5: 'Discontinuous Low Density Urban Fabric (S.L. : 10% - 30%)',\n",
       " 6: 'Discontinuous Medium Density Urban Fabric (S.L. : 30% - 50%)',\n",
       " 7: 'Discontinuous Very Low Density Urban Fabric (S.L. < 10%)',\n",
       " 8: 'Fast transit roads and associated land',\n",
       " 9: 'Forests',\n",
       " 10: 'Green urban areas',\n",
       " 11: 'Industrial, commercial, public, military and private units',\n",
       " 12: 'Isolated Structures',\n",
       " 13: 'Land without current use',\n",
       " 14: 'Mineral extraction and dump sites',\n",
       " 15: 'Other roads and associated land',\n",
       " 16: 'Port areas',\n",
       " 17: 'Railways and associated land',\n",
       " 18: 'Sports and leisure facilities',\n",
       " 19: 'Water bodies'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def get_city_center(shapefile):\n",
    "    geolocator = Nominatim()\n",
    "    country_code = shapefile.split(\"/\")[-1].split(\"_\")[0][:2]\n",
    "    city = \" \".join(shapefile.split(\"/\")[-1].split(\"_\")[1:]).split(\".\")[0]\n",
    "    location = geolocator.geocode(city + \",\" + country_code)\n",
    "    if location is None:\n",
    "        return None, None\n",
    "    latlon = (location.latitude, location.longitude)\n",
    "    return latlon, country_code\n",
    "\n",
    "\n",
    "def filter_gdf_by_polygon(gdf, polygon):\n",
    "    spatial_index = gdf.sindex\n",
    "    possible_matches_index = list(spatial_index.intersection(polygon.bounds))\n",
    "    possible_matches = gdf.iloc[possible_matches_index]\n",
    "    precise_matches = possible_matches[possible_matches.intersects(polygon)]\n",
    "    return precise_matches\n",
    "\n",
    "\n",
    "def filter_gdf_by_centered_window(gdf0, center=None, window=None):\n",
    "    if window is None:\n",
    "        return gdf0\n",
    "    else:\n",
    "        latmin, lonmin, latmax, lonmax = satimg.bounding_box_at_location(center, window)\n",
    "        pbox = Polygon([(lonmin,latmin), (lonmax,latmin), (lonmax,latmax), (lonmin,latmax)])\n",
    "        return filter_gdf_by_polygon(gdf0, pbox)\n",
    "    \n",
    "    \n",
    "def construct_class_raster(gdf, bbox, grid_size=(100,100)):\n",
    "    grid_size_lon, grid_size_lat = grid_size\n",
    "    latmin_grid, lonmin_grid, latmax_grid, lonmax_grid = bbox\n",
    "    latv = np.linspace(latmin_grid, latmax_grid, grid_size_lat+1)\n",
    "    lonv = np.linspace(lonmin_grid, lonmax_grid, grid_size_lon+1)\n",
    "    \n",
    "    raster = np.zeros((grid_size_lon, grid_size_lat, len(classes)))\n",
    "    locations = []\n",
    "    for i in range(len(lonv)-1):\n",
    "        clear_output(wait=True)\n",
    "        print \"%d / %d\"%(i, len(lonv)-1)\n",
    "        for j in range(len(latv)-1):\n",
    "            cell_poly = Polygon([(lonv[i],latv[j]), (lonv[i+1],latv[j]), \\\n",
    "                                 (lonv[i+1],latv[j+1]), (lonv[i],latv[j+1])])\n",
    "            gdf_frame = filter_gdf_by_polygon(gdf, cell_poly)\n",
    "            if len(gdf_frame) == 0:\n",
    "                continue\n",
    "            areas_per_class = gdf_frame.groupby(\"ITEM\")\\\n",
    "                                .apply(lambda x: x.intersection(cell_poly)\\\n",
    "                                       .apply(lambda y: y.area*(6400**2)).sum())\n",
    "            classified_area = areas_per_class.sum()\n",
    "            if classified_area > 0:\n",
    "                areas_per_class = areas_per_class / float(classified_area) \n",
    "                raster[i,j,:] = [areas_per_class[label2class[k]] if label2class[k] in areas_per_class\\\n",
    "                                 else 0 for k in range(len(classes))]  \n",
    "                # also save sampling locations\n",
    "                # only if we can get ground truth label for the cell\n",
    "                cell_class = areas_per_class.argmax()\n",
    "                loc = (i, j, \n",
    "                       cell_poly.centroid.xy[0][0], \n",
    "                       cell_poly.centroid.xy[1][0], \n",
    "                       cell_class)\n",
    "                locations.append(loc)\n",
    "    \n",
    "    locations = pd.DataFrame(locations, \\\n",
    "                    columns=[\"grid-i\", \"grid-j\", \"lon\", \"lat\", \"class\"])\n",
    "    return raster, locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "grid_cell = 100\n",
    "grid_size = (grid_cell, grid_cell)\n",
    "window_km_vec = [25, 30, 50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fn_generate_stats(shapefile):\n",
    "    city = \" \".join(shapefile.split(\"/\")[-1].split(\"_\")[1:]).replace(\".shp\",\"\")\n",
    "    \n",
    "    # weird issues with several cities, skip\n",
    "    if city in [\"limoges\", \"linz\"]:\n",
    "        return \"Error for city %s\"%city\n",
    "    \n",
    "    print \"Processing %s\"%city\n",
    "    \n",
    "    savedir = \"%s/%s/\"%(outPath, city)\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "\n",
    "    if len([x for x in os.listdir(savedir) if 'raster' in x])==3:\n",
    "        return \"Already processed!\"\n",
    "   \n",
    "    gdf, prj = load_shapefile(shapefile)\n",
    "    if gdf is None:\n",
    "        return \"Error reading shapefile %s\"%shapefile\n",
    "        \n",
    "    city_center, country_code = get_city_center(shapefile)\n",
    "    lonmin, latmin, lonmax, latmax = get_bounds(gdf)\n",
    "    bounds_gdf = Polygon([(lonmin,latmin), (lonmax,latmin), (lonmax,latmax), (lonmin,latmax)])\n",
    "\n",
    "    if city_center is None:\n",
    "        city_center = ((latmin+latmax)/2.0, (lonmin+lonmax)/2.0)\n",
    "\n",
    "    # there's some weird issue with the shapefile for Graz\n",
    "    # lat and lon are inverted?\n",
    "    if city in [\"graz\"]: #not bounds_gdf.contains(Point(city_center[::-1])):\n",
    "        city_center = ((latmin+latmax)/2.0, (lonmin+lonmax)/2.0)\n",
    "        gdf['geometry'] = gdf['geometry'].apply(\\\n",
    "                lambda p: Polygon((lon,lat) \\\n",
    "                    for (lon,lat) in zip(p.exterior.coords.xy[1], p.exterior.coords.xy[0])))\n",
    "    \n",
    "    # compute spatial extent of city and fraction of land classified\n",
    "    L, frac_classified = compute_stats(gdf, prj=prj)\n",
    "    df = pd.DataFrame([L, frac_classified], \\\n",
    "                      index=[\"spatial extent\", \"pct land classified\"]).T\n",
    "    df.to_csv(\"%s/basic_stats.csv\"%savedir)\n",
    "        \n",
    "    for window_km in window_km_vec:\n",
    "        window = (window_km, window_km)\n",
    "        gdf_window = filter_gdf_by_centered_window(gdf, center=city_center, window=window)\n",
    "        \n",
    "        # compute stats\n",
    "        class_coverage_by_area = gdf_window.groupby(\"ITEM\").apply(\\\n",
    "                                lambda x: x[\"SHAPE_AREA\"].sum())/float(window[0]*window[1])\n",
    "        class_coverage_by_poly= gdf_window.groupby(\"ITEM\").apply(len)/ gdf.groupby(\"ITEM\").apply(len)\n",
    "        class_coverage_by_area_classified = gdf_window.groupby(\"ITEM\").apply(\\\n",
    "                                                lambda x: x['SHAPE_AREA'].sum()) / gdf_window['SHAPE_AREA'].sum()\n",
    "    \n",
    "        # format and save stats\n",
    "        stats_df = pd.concat([class_coverage_by_area, class_coverage_by_poly, class_coverage_by_area_classified], axis=1)\n",
    "        stats_df.columns = [\"pct area\", \"pct polygons\", \"pct classified area\"]\n",
    "        stats_df['window km'] = window_km\n",
    "        stats_df = stats_df.ix[classes]\n",
    "        stats_df.to_csv(\"%s/stats_class_window_%d.csv\"%(savedir,window_km))\n",
    "        \n",
    "        # compute raster for given window size\n",
    "        bbox = satimg.bounding_box_at_location(city_center, window)\n",
    "        raster, locations_df = construct_class_raster(gdf_window, bbox, grid_size=grid_size)\n",
    "        np.savez_compressed(\"%s/ground_truth_class_raster_%d.npz\"%(savedir,window_km), raster)\n",
    "        locations_df.to_csv(\"%s/sample_locations_raster_%d.csv\"%(savedir,window_km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# city_center, country_code = get_city_center(shapefile)\n",
    "# lonmin, latmin, lonmax, latmax = get_bounds(gdf)\n",
    "# bounds_gdf = Polygon([(lonmin,latmin), (lonmax,latmin), (lonmax,latmax), (lonmin,latmax)])\n",
    "# window = (window_km_vec[0], window_km_vec[0])\n",
    "# gdf_window = filter_gdf_by_centered_window(gdf, center=city_center, window=window)\n",
    "# bbox = satimg.bounding_box_at_location(city_center, window)\n",
    "# raster, locations_df = construct_class_raster(gdf_window, bbox, grid_size=grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = lbv.map_async(fn_generate_stats, shapefiles.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# res.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate locations to extract imagery at\n",
    "\n",
    "Our sampling strategy has the following goals:\n",
    "* ensure that a uniform $100 \\times 100 ~ (25km \\times 25km)$ \"main grid\" is completely sampled (except for where there are no ground truth polygons). We generate samples in this grid first, and assign the ground truth label of the image sampled in each grid cell to the class of the polygon that has the maximum intersection area with that cell; \n",
    "* ensure that the resulting dataset is balanced with respect to the land use classes. The trouble is that the classes are highly imbalanced among the polygons in the dataset (e.g., many more polygons are agricultural land and isolated structures than airports).\n",
    "* sample additional polygons apart from the ones in the initial grid, such that only polygons above a certain threshold size are considered (so that we can ensure that the sampled images contain a large enough area of the class they represent). \n",
    "* to ensure higher match between labels and sampled images, sample more images from polygons of larger areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "img_area = (224 * 1.19/ 1000)**2 # in km^2, at zoom level 17\n",
    "thresh_frac = 0.25 # at least <thresh_frac> % of the image should be covered by a polygon of a given class\n",
    "thresh_area = img_area * thresh_frac  \n",
    "# print \"Threshold area: %2.2f km^2\"%thresh_area\n",
    "\n",
    "n_classes = len(classes)\n",
    "\n",
    "N_SAMPLES_PER_CITY  = 25000\n",
    "N_SAMPLES_PER_CLASS = N_SAMPLES_PER_CITY / n_classes\n",
    "MAX_SAMPLES_PER_POLY= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12292 4674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ITEM\n",
       "Agricultural + Semi-natural areas + Wetlands                     594\n",
       "Airports                                                           4\n",
       "Construction sites                                                94\n",
       "Continuous Urban Fabric (S.L. > 80%)                            2245\n",
       "Discontinuous Dense Urban Fabric (S.L. : 50% -  80%)             284\n",
       "Discontinuous Medium Density Urban Fabric (S.L. : 30% - 50%)       2\n",
       "Fast transit roads and associated land                             3\n",
       "Forests                                                          117\n",
       "Green urban areas                                                123\n",
       "Industrial, commercial, public, military and private units       857\n",
       "Isolated Structures                                                2\n",
       "Land without current use                                          69\n",
       "Mineral extraction and dump sites                                 42\n",
       "Other roads and associated land                                    7\n",
       "Railways and associated land                                      29\n",
       "Sports and leisure facilities                                     68\n",
       "Water bodies                                                     134\n",
       "dtype: int64"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_sel = gdf[gdf.SHAPE_AREA>=thresh_area]\n",
    "print len(gdf), len(gdf_sel)\n",
    "\n",
    "gdf_sel.groupby(\"ITEM\").apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "def fn_select_polygons(df, n_samples=1000, max_samples=None):    \n",
    "    samples_per_poly = (df.SHAPE_AREA/float(df.SHAPE_AREA.min()))\\\n",
    "                            .astype(int)\n",
    "    # print df.ITEM.iloc[0]\n",
    "    if samples_per_poly.sum() > n_samples:\n",
    "        pvec = np.array([0.0, 0.2, 0.5, 0.7, 0.9, 0.95, 1])\n",
    "        bins = np.percentile(samples_per_poly, pvec*100)\n",
    "        cnts, _ = np.histogram(samples_per_poly, bins)\n",
    "\n",
    "        ret = []\n",
    "        x = samples_per_poly\n",
    "        for i in range(len(bins)-1):\n",
    "            if cnts[i] == 0:\n",
    "                continue\n",
    "            y = x[(x>=bins[i]) & (x<bins[i+1])] if i<len(bins)-2 \\\n",
    "                    else x[(x>=bins[i]) & (x<=bins[i+1])]\n",
    "            # print i, (bins[i], bins[i+1]), cnts[i], pvec[i+1], len(x[(x>=bins[i]) & (x<=bins[i+1])])\n",
    "            y = y.sample(frac=pvec[i+1])\n",
    "            ret.append(y)\n",
    "        ret = pd.concat(ret)\n",
    "        ret_scaled = (ret.astype(float) / ret.sum() * n_samples)\\\n",
    "                        .apply(np.ceil).astype(int)\n",
    "        ret_df = df.ix[ret_scaled.index]\n",
    "        ret_df['samples'] = ret_scaled.values\n",
    "    else:\n",
    "        ret_df = df\n",
    "        ret_df['samples'] = samples_per_poly.values\n",
    "    \n",
    "    # clamp # samples per polygon if specified\n",
    "    if max_samples is not None:\n",
    "        ret_df['samples'] = ret_df['samples'].apply(\\\n",
    "                                    lambda x: min([x, max_samples]))\n",
    "    ret_df['samples'] = ret_df['samples'].astype(int)\n",
    "    return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "select_polygons = gdf_sel.groupby(\"ITEM\")\\\n",
    "    .apply(lambda x: fn_select_polygons(x, n_samples=N_SAMPLES_PER_CLASS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Agricultural + Semi-natural areas + Wetlands</th>\n",
       "      <td>797</td>\n",
       "      <td>594.0</td>\n",
       "      <td>1479.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Airports</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>433.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Construction sites</th>\n",
       "      <td>181</td>\n",
       "      <td>94.0</td>\n",
       "      <td>287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Continuous Urban Fabric (S.L. &gt; 80%)</th>\n",
       "      <td>7126</td>\n",
       "      <td>2245.0</td>\n",
       "      <td>2263.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discontinuous Dense Urban Fabric (S.L. : 50% -  80%)</th>\n",
       "      <td>592</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discontinuous Low Density Urban Fabric (S.L. : 10% - 30%)</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discontinuous Medium Density Urban Fabric (S.L. : 30% - 50%)</th>\n",
       "      <td>25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fast transit roads and associated land</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forests</th>\n",
       "      <td>128</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Green urban areas</th>\n",
       "      <td>329</td>\n",
       "      <td>123.0</td>\n",
       "      <td>585.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Industrial, commercial, public, military and private units</th>\n",
       "      <td>1978</td>\n",
       "      <td>857.0</td>\n",
       "      <td>1568.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Isolated Structures</th>\n",
       "      <td>310</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Land without current use</th>\n",
       "      <td>359</td>\n",
       "      <td>69.0</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mineral extraction and dump sites</th>\n",
       "      <td>59</td>\n",
       "      <td>42.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other roads and associated land</th>\n",
       "      <td>50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Railways and associated land</th>\n",
       "      <td>34</td>\n",
       "      <td>29.0</td>\n",
       "      <td>349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports and leisure facilities</th>\n",
       "      <td>140</td>\n",
       "      <td>68.0</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water bodies</th>\n",
       "      <td>173</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1302.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0       1       2\n",
       "Agricultural + Semi-natural areas + Wetlands         797   594.0  1479.0\n",
       "Airports                                               4     4.0   433.0\n",
       "Construction sites                                   181    94.0   287.0\n",
       "Continuous Urban Fabric (S.L. > 80%)                7126  2245.0  2263.0\n",
       "Discontinuous Dense Urban Fabric (S.L. : 50% - ...   592   284.0  1180.0\n",
       "Discontinuous Low Density Urban Fabric (S.L. : ...     4     NaN     NaN\n",
       "Discontinuous Medium Density Urban Fabric (S.L....    25     2.0     4.0\n",
       "Fast transit roads and associated land                 3     3.0     6.0\n",
       "Forests                                              128   117.0  1287.0\n",
       "Green urban areas                                    329   123.0   585.0\n",
       "Industrial, commercial, public, military and pr...  1978   857.0  1568.0\n",
       "Isolated Structures                                  310     2.0     2.0\n",
       "Land without current use                             359    69.0   131.0\n",
       "Mineral extraction and dump sites                     59    42.0   123.0\n",
       "Other roads and associated land                       50     7.0  1013.0\n",
       "Railways and associated land                          34    29.0   349.0\n",
       "Sports and leisure facilities                        140    68.0   183.0\n",
       "Water bodies                                         173   134.0  1302.0"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([gdf.groupby(\"ITEM\").apply(len),\n",
    "           gdf_sel.groupby(\"ITEM\").apply(len),\n",
    "           select_polygons.groupby(\"ITEM\").apply(lambda x: x['samples'].sum())\n",
    "          ], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "\n",
    "def fn_sample_locations(df, sample_on_boundary=False):\n",
    "    polygons = df['geometry']\n",
    "    nsamples = df['samples']\n",
    "    \n",
    "    if not sample_on_boundary:\n",
    "        centroids = np.array([(p.centroid.coords.xy[0][0], p.centroid.coords.xy[1][0]) \\\n",
    "                      for p in polygons])    \n",
    "        idx = nsamples > 1\n",
    "        if idx.sum()>0:\n",
    "            polygons = polygons[idx]\n",
    "            nsamples = nsamples[idx]\n",
    "            locs = [satimg.generate_locations_within_polygon(p, nSamples=m-1, strict=True) \\\n",
    "                    for p,m in zip(polygons, nsamples)]\n",
    "            locs = np.vstack(locs).squeeze()\n",
    "            locs = np.vstack([locs, centroids])\n",
    "        else:\n",
    "            locs = centroids\n",
    "    else:\n",
    "        boundaries= [zip(p.exterior.coords.xy[0], p.exterior.coords.xy[1]) \\\n",
    "                     for p in polygons]\n",
    "        locs = np.array([b[l] for b,m in zip(boundaries,nsamples) \\\n",
    "                         for l in np.random.choice(np.arange(0,len(b)), min([len(b),m]))])\n",
    "    ret = pd.DataFrame(locs, columns=[\"lon\", \"lat\"])\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12195, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ITEM\n",
       "Agricultural + Semi-natural areas + Wetlands                    1479\n",
       "Airports                                                         433\n",
       "Construction sites                                               287\n",
       "Continuous Urban Fabric (S.L. > 80%)                            2263\n",
       "Discontinuous Dense Urban Fabric (S.L. : 50% -  80%)            1180\n",
       "Discontinuous Medium Density Urban Fabric (S.L. : 30% - 50%)       4\n",
       "Fast transit roads and associated land                             6\n",
       "Forests                                                         1287\n",
       "Green urban areas                                                585\n",
       "Industrial, commercial, public, military and private units      1568\n",
       "Isolated Structures                                                2\n",
       "Land without current use                                         131\n",
       "Mineral extraction and dump sites                                123\n",
       "Other roads and associated land                                 1013\n",
       "Railways and associated land                                     349\n",
       "Sports and leisure facilities                                    183\n",
       "Water bodies                                                    1302\n",
       "dtype: int64"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = select_polygons.groupby(\"ITEM\")\\\n",
    "                .apply(lambda x: fn_sample_locations(x,\n",
    "                        sample_on_boundary = ('road' in x['ITEM'].iloc[0].lower() or 'railway' in x['ITEM'].iloc[0].lower())\n",
    "            ))\n",
    "\n",
    "    \n",
    "# locations.to_csv(\"%s/samples_%s.csv\"%(outPath, city))\n",
    "\n",
    "print locations.shape\n",
    "\n",
    "locations.reset_index().groupby(\"ITEM\").apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate locations for all cities in the Urban Atlas dataset\n",
    "\n",
    "It does take ~30 seconds for each city,so this will take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adalbert/data/urban-atlas/extracted-data'"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fn_generate_locations(shapefile):\n",
    "    city = \" \".join(shapefile.split(\"/\")[-1].split(\"_\")[1:]).replace(\".shp\",\"\")\n",
    "    \n",
    "    savefile = \"%s/%s/additional_sample_locations.csv\"%(outPath, city)\n",
    "    if os.path.exists(savefile):\n",
    "        return \"Sample file already exists %s\" % savefile\n",
    "    \n",
    "    gdf, prj = load_shapefile(shapefile)\n",
    "    gdf_sel = gdf[gdf.SHAPE_AREA>=thresh_area]\n",
    "\n",
    "    # select polygons to sample\n",
    "    select_polygons = gdf_sel.groupby(\"ITEM\")\\\n",
    "                        .apply(lambda x: fn_select_polygons(x, n_samples=N_SAMPLES_PER_CLASS))\n",
    "    if \"ITEM\" not in select_polygons.columns:\n",
    "        select_polygons.reset_index(inplace=True)\n",
    "    \n",
    "    # make sure all polygons are ok\n",
    "    # some polygons have their geometries messed up in the previous step??\n",
    "    select_polygons['geometry'] = select_polygons['geometry'].apply(lambda p: p.buffer(0) if not p.is_valid else p)\n",
    "    \n",
    "    # sample locations from each polygon\n",
    "    locations = select_polygons.groupby(\"ITEM\")\\\n",
    "                .apply(lambda x: fn_sample_locations(x,\n",
    "                        sample_on_boundary = 'road' in x['ITEM'].iloc[0].lower() \\\n",
    "                                                or 'railway' in x['ITEM'].iloc[0].lower()))\n",
    "    \n",
    "    print \"--> selected %d sampling locations.\"%len(locations)\n",
    "    locations.to_csv(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_locs = lbv.map_async(fn_generate_locations, shapefiles.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_locs.progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute statistics on locations generated for a few cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities = [\"bucuresti\", \"berlin\", \"barcelona\", \"paris\", \"athina\", \\\n",
    "          \"firenze\", \"dublin\", \"london\", \"tallinn\", \"bremen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_locations():\n",
    "    grid_locations_df = pd.read_csv(\"sample_locations_raster_25.csv\").drop(\"Unnamed: 0\", 1)\n",
    "    more_locations_df = pd.read_csv(\"additional_sample_locations.csv\")\\\n",
    "                            .rename(columns={\"ITEM\":\"class\"})\\\n",
    "                            .drop(\"Unnamed: 1\", 1)\n",
    "    print \"Grid samples: %d. Additional samples: %d\" % \\\n",
    "            (len(grid_locations_df), len(more_locations_df))\n",
    "        \n",
    "    more_locations_df['grid-i'] = np.nan\n",
    "    more_locations_df['grid-j'] = np.nan\n",
    "    columns = [\"lon\", \"lat\", \"grid-i\", \"grid-j\", \"class\"]\n",
    "    locations = pd.concat([grid_locations_df[columns], more_locations_df[columns]])\n",
    "    locations = locations.reset_index().drop(\"index\", 1)\n",
    "    \n",
    "    return locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucuresti Grid samples: 9745. Additional samples: 12213\n",
      "berlin Grid samples: 10000. Additional samples: 49325\n",
      "barcelona Grid samples: 5944. Additional samples: 18299\n",
      "athina Grid samples: 8738. Additional samples: 18544\n",
      "dublin Grid samples: 8514. Additional samples: 22391\n",
      "tallinn Grid samples: 7000. Additional samples: 16414\n"
     ]
    }
   ],
   "source": [
    "stats_df = []\n",
    "for city in cities:\n",
    "    workdir = \"%s/%s\" % (outPath, city)\n",
    "    os.chdir(workdir)\n",
    "    if os.path.isdir(\"./img\"):\n",
    "        print city,\n",
    "        locations = load_locations()\n",
    "        stats_df.append(locations)\n",
    "stats_df = pd.concat(stats_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187127, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = np.array([f for c in cities for f in glob.glob(outPath + \"%s/img/*/*.jpg\"%c)])\n",
    "files = [f for f in files if not ('grid' in os.path.basename(f))]\n",
    "files_df =  pd.DataFrame(files).rename(columns={0:\"filename\"})\n",
    "files_df['class'] = files_df['filename'].apply(lambda x: x.split(\"/\")[-2])\n",
    "files_df['city'] = files_df['filename'].apply(lambda x: x.split(\"/\")[-4])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
